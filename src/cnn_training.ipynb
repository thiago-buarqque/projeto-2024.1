{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/evry/Desktop/master-degree/repositories/vision-anomaly/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evry/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "from torchsummary import summary\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "\n",
    "from model.model import Model\n",
    "\n",
    "from src.data_loader.data_loader import MVTec\n",
    "from progressbar import Bar, DynamicMessage, ProgressBar, ETA\n",
    "\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasad.data_loader import MVTecTrainDataset\n",
    "from tasad.data_loader_test import MVTecTestDataset\n",
    "\n",
    "dataset_root_path = \"/home/evry/Desktop/master-degree/repositories/two-stage-coarse-to-fine-image-anomaly-segmentation-and-detection-model/data/images\"\n",
    "anomaly_path = \"/home/evry/Desktop/master-degree/repositories/two-stage-coarse-to-fine-image-anomaly-segmentation-and-detection-model/data/anomaly/images\"\n",
    "# dataset_root_path = \"/home/evry/Desktop/master-degree/dataset/BTech_Dataset_transformed\"\n",
    "\n",
    "def read_data(class_name: str, batch_size=1):\n",
    "    train_dataset = MVTecTrainDataset(root_dir=dataset_root_path + f\"/{class_name}/train/\", anomaly_source_path=anomaly_path, resize_shape=[256, 256])\n",
    "    test_dataset = MVTecTestDataset(root_dir=dataset_root_path + f\"/{class_name}/test/\", resize_shape=[256, 256])\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_comparison(\n",
    "    class_name: str, \n",
    "    file_name:str, \n",
    "    image, \n",
    "    mask, \n",
    "    reconstruction, \n",
    "    ssim_map, \n",
    "    fas_input, \n",
    "    fas_output, \n",
    "    processed_fas_output, \n",
    "    save_fig: bool, \n",
    "    summary_writer: SummaryWriter, \n",
    "    epoch,\n",
    "    path: str = \"../runs/tasad-vitcnn\"\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        _input_image = image.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "        _mask = mask.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "        _fas_input = fas_input.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "        _fas_output = fas_output.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "        _ssim_map = ssim_map.cpu().numpy()[0][0]\n",
    "        _processed_fas_output = processed_fas_output.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "        \n",
    "        # Create a heatmap from the normalized SSIM map\n",
    "        heatmap = cv2.applyColorMap((_ssim_map * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "        # Convert _input_image to uint8\n",
    "        _input_image_uint8 = (_input_image * 255).astype(np.uint8)\n",
    "\n",
    "        # Overlay the heatmap on the original input image\n",
    "        overlay = cv2.addWeighted(_input_image_uint8, 0.4, heatmap, 0.6, 0)\n",
    "\n",
    "        # Plot the results\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3))\n",
    "\n",
    "        ax1 = fig.add_subplot(171)\n",
    "        ax1.imshow(_input_image, cmap='gray')\n",
    "        ax1.set_title('ViT-CNN entrada')\n",
    "        ax1.axis(\"off\")\n",
    "        \n",
    "        reconstruction_norm = (reconstruction - reconstruction.min()) / (reconstruction.max() - reconstruction.min())\n",
    "        \n",
    "        ax2 = fig.add_subplot(172)\n",
    "        ax2.imshow(reconstruction_norm.cpu().numpy()[0].transpose(1, 2, 0))\n",
    "        ax2.set_title('ViT-CNN saída')\n",
    "        ax2.axis(\"off\")\n",
    "        \n",
    "        ax3 = fig.add_subplot(173)\n",
    "        ax3.imshow(overlay)\n",
    "        ax3.set_title('ViT-CNN mapa SSIM')\n",
    "        ax3.axis(\"off\")\n",
    "        \n",
    "        ax4 = fig.add_subplot(174)\n",
    "        ax4.imshow(_mask, cmap='gray')\n",
    "        ax4.set_title('Padrão ouro')\n",
    "        ax4.axis(\"off\")\n",
    "        \n",
    "        ax5 = fig.add_subplot(175)\n",
    "        ax5.imshow(_fas_input)\n",
    "        ax5.set_title('Entrada FAS')\n",
    "        ax5.axis(\"off\")\n",
    "        \n",
    "        ax6 = fig.add_subplot(176)\n",
    "        ax6.imshow(_fas_output, cmap='gray')\n",
    "        ax6.set_title('Saída FAS')\n",
    "        ax6.axis(\"off\")\n",
    "        \n",
    "        ax6 = fig.add_subplot(177)\n",
    "        ax6.imshow(_processed_fas_output, cmap='gray')\n",
    "        ax6.set_title('Saída FAS binária')\n",
    "        ax6.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # if plot:\n",
    "        #     plt.show()\n",
    "        \n",
    "        path += f\"/{class_name}/plots/\"\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        if save_fig:\n",
    "            fig.savefig(path + file_name)        \n",
    "\n",
    "        summary_writer.add_figure('plot', fig, epoch)\n",
    "        \n",
    "        fig.clear()\n",
    "        plt.close()\n",
    "        plt.cla()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vitcnn_output_mask(input_batch, vitcnn):\n",
    "    _, reconstruction = vitcnn(input_batch)\n",
    "    \n",
    "    SSIM = StructuralSimilarityIndexMeasure(return_full_image=True).cpu()\n",
    "    \n",
    "    ssim_value, ssim_map = SSIM(input_batch.cpu(), reconstruction.cpu())\n",
    "    \n",
    "    norm_ssim_map = (ssim_map - ssim_map.min()) / (ssim_map.max() - ssim_map.min())\n",
    "    \n",
    "    mean_tensor = norm_ssim_map.mean(dim=1, keepdim=True)\n",
    "\n",
    "    norm_ssim_map = mean_tensor.expand(-1, 3, -1, -1)\n",
    "    \n",
    "    binary_ssim_map = torch.where(norm_ssim_map > 0.9, torch.zeros_like(norm_ssim_map), torch.ones_like(norm_ssim_map))\n",
    "    \n",
    "    return input_batch * binary_ssim_map.cuda(), reconstruction, ssim_value, ssim_map, binary_ssim_map.cpu().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt \n",
    "from tasad.tasad_model import TasadModel\n",
    "from tasad.utils.utilts_custom_class import *\n",
    "from tasad.utils.utilts_func import *\n",
    "import cv2\n",
    "from typing import Union\n",
    "from datetime import datetime\n",
    "from progressbar import Bar, DynamicMessage, ProgressBar, ETA\n",
    "\n",
    "def test_segmentation_model(\n",
    "    vitcnn: Model,\n",
    "    class_name: str,\n",
    "    dataloader,\n",
    "    epoch: int,\n",
    "    gpu_id,\n",
    "    fas_model: TasadModel,\n",
    "    visualizer: SummaryWriter = None,\n",
    "    print_logs: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Test the segmentation model on the MVTec test dataset.\n",
    "    \n",
    "    Args:\n",
    "    segmentation_model (TasadModel): The main segmentation model to be tested.\n",
    "    class_name (str): The name of the class to be tested.\n",
    "    data_path (str): Path to the dataset.\n",
    "    epoch (int): Current epoch number.\n",
    "    gpu_id: GPU ID for CUDA device.\n",
    "    fas_model (TasadModel, optional): Additional model for further segmentation (if any).\n",
    "    visualizer (TensorboardVisualizer, optional): Tensorboard visualizer for performance plotting.\n",
    "    print_logs (bool, optional): Flag to print logs.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: AP, AP per pixel, AUROC, AUROC per pixel\n",
    "    \"\"\"\n",
    "    \n",
    "    cuda_device = torch.device(f'cuda:{gpu_id}')\n",
    "    img_dimension = 256\n",
    "    image_index = 0\n",
    "\n",
    "    # Initialize the FAS model if provided\n",
    "    fas_model.cuda(cuda_device)\n",
    "    fas_model.eval()\n",
    "\n",
    "    dataset = dataloader.dataset\n",
    "    total_pixel_scores = np.zeros((img_dimension * img_dimension * len(dataset)))\n",
    "    total_gt_pixel_scores = np.zeros((img_dimension * img_dimension * len(dataset)))\n",
    "    mask_count = 0\n",
    "\n",
    "    anomaly_score_gt = []\n",
    "    anomaly_score_prediction = []\n",
    "    \n",
    "    widgets = [\n",
    "        DynamicMessage('test'),\n",
    "        Bar(marker='=', left='[', right=']'),\n",
    "        ' ', ETA(),\n",
    "    ]\n",
    "\n",
    "    with ProgressBar(widgets=widgets, max_value=len(dataset)) as progress_bar:\n",
    "        saved_in_this_epoch = False\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            vitcnn_input = sample_batched[\"augmented_image\"].cuda(cuda_device)\n",
    "            original_image = plt.imread(dataloader.dataset.images[i_batch]) \n",
    "            resized_original_image = cv2.resize(original_image, (img_dimension, img_dimension))\n",
    "\n",
    "            has_anomaly = sample_batched[\"has_anomaly\"].detach().numpy()\n",
    "            anomaly_score_gt.append(has_anomaly)\n",
    "            ground_truth_mask = sample_batched[\"anomaly_mask\"]\n",
    "            \n",
    "            ground_truth_mask_np = ground_truth_mask.detach().numpy()[0, :, :, :].transpose((1, 2, 0))\n",
    "                \n",
    "            fas_input, vitcnn_reconstruction, vitcnn_ssim_value, vitcnn_ssim_map, vitcnn_binary_ssim_map = \\\n",
    "                    get_vitcnn_output_mask(vitcnn_input, vitcnn)\n",
    "                \n",
    "            fas_output = fas_model(fas_input)\n",
    "            \n",
    "            vitcnn_binary_ssim_map = vitcnn_binary_ssim_map.to(fas_output.device)\n",
    "            \n",
    "            processed_fas_output = fas_output * vitcnn_binary_ssim_map.mean(dim=1, keepdim=True)\n",
    "            \n",
    "            processed_fas_output = torch.where(fas_output < 0.3, torch.zeros_like(processed_fas_output), torch.ones_like(processed_fas_output))\n",
    "            \n",
    "            if has_anomaly[0] and not saved_in_this_epoch:\n",
    "                saved_in_this_epoch = True\n",
    "                \n",
    "                save_comparison(\n",
    "                    class_name=class_name,\n",
    "                    file_name=f\"test_sample_epoch_{epoch}.jpg\",\n",
    "                    image=vitcnn_input,\n",
    "                    mask=ground_truth_mask,\n",
    "                    reconstruction=vitcnn_reconstruction,\n",
    "                    ssim_map=vitcnn_ssim_map,\n",
    "                    fas_input=fas_input,\n",
    "                    fas_output=fas_output,\n",
    "                    processed_fas_output=processed_fas_output,\n",
    "                    save_fig=True,\n",
    "                    summary_writer=visualizer,\n",
    "                    epoch=epoch,\n",
    "                    path=f\"../runs/tasad-vitcnn/{class_name}/test\")\n",
    "                \n",
    "            # output_mask_np = processed_fas_output.detach().cpu().numpy()\n",
    "            # output = fas_output                                                                                                                                   \n",
    "            \n",
    "            # fas_input = torch.tensor(seg_module(vitcnn_input, cas_output, th_pix=0.95, th_val=30)).cuda(cuda_device)\n",
    "            # fas_output = fas_model(fas_input)\n",
    "            # combined_output = fas_output + cas_output\n",
    "\n",
    "            output_mask_np = processed_fas_output[0, 0, :, :].detach().cpu().numpy()\n",
    "            output = fas_output                                                                                                                                   \n",
    "           \n",
    "            try:\n",
    "                fas_input_np = fas_input.detach().cpu().numpy()[0, :, :, :].transpose((1, 2, 0))   \n",
    "                fas_input_rgb = cv2.cvtColor(fas_input_np, cv2.COLOR_BGR2RGB)\n",
    "                query_image_rgb = cv2.cvtColor(resized_original_image, cv2.COLOR_BGR2RGB)\n",
    "                out_mask_fas_np = abs(fas_output.detach().cpu().numpy()[0, :, :, :].transpose((1, 2, 0))[:,:,0])/torch.max(fas_output).item()\n",
    "                # out_mask_cas_np = abs(cas_output.detach().cpu().numpy()[0, :, :, :].transpose((1, 2, 0))[:,:,0])/torch.max(cas_output).item()\n",
    "                combined_output_normalized = abs(out_mask_fas_np)\n",
    "                combined_output_normalized = combined_output_normalized/np.max(combined_output_normalized)\n",
    "                \n",
    "                all_images = [query_image_rgb,  ground_truth_mask_np, fas_input_rgb, out_mask_fas_np, combined_output_normalized] \n",
    "                # Image saving function can be implemented here if needed\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            image_index += 1\n",
    "            averaged_output_mask = torch.nn.functional.avg_pool2d(output, 21, stride=1, padding=21 // 2).cpu().detach().numpy()\n",
    "            image_score = np.max(averaged_output_mask)\n",
    "\n",
    "            anomaly_score_prediction.append(image_score)\n",
    "\n",
    "            flat_gt_mask = ground_truth_mask_np.flatten()\n",
    "            flat_output_mask = output_mask_np.flatten()\n",
    "            \n",
    "            total_pixel_scores[mask_count * img_dimension * img_dimension:(mask_count + 1) * img_dimension * img_dimension] = flat_output_mask\n",
    "            total_gt_pixel_scores[mask_count * img_dimension * img_dimension:(mask_count + 1) * img_dimension * img_dimension] = flat_gt_mask\n",
    "            mask_count += 1\n",
    "            \n",
    "            progress_bar.update(\n",
    "                i_batch,\n",
    "                test=f\"({i_batch}/{len(dataset)}) Class {class_name} \")\n",
    "\n",
    "    anomaly_score_prediction = np.array(anomaly_score_prediction)\n",
    "    anomaly_score_gt = np.array(anomaly_score_gt)\n",
    "    auroc = roc_auc_score(anomaly_score_gt, anomaly_score_prediction)\n",
    "    ap = average_precision_score(anomaly_score_gt, anomaly_score_prediction)\n",
    "\n",
    "    total_gt_pixel_scores = total_gt_pixel_scores.astype(np.uint8)\n",
    "    total_gt_pixel_scores = total_gt_pixel_scores[:img_dimension * img_dimension * mask_count]\n",
    "    total_pixel_scores = total_pixel_scores[:img_dimension * img_dimension * mask_count]\n",
    "    \n",
    "    auroc_pixel = roc_auc_score(total_gt_pixel_scores, total_pixel_scores)\n",
    "    ap_pixel = average_precision_score(total_gt_pixel_scores, total_pixel_scores)\n",
    "        \n",
    "    if visualizer:\n",
    "        visualizer.add_scalar(\"test_AP_pixel\", ap_pixel, epoch)\n",
    "        visualizer.add_scalar(\"test_AUROC_pixel\", auroc_pixel, epoch)\n",
    "        visualizer.add_scalar(\"test_AUROC\", auroc, epoch)\n",
    "        visualizer.add_scalar(\"test_AP\", ap, epoch)\n",
    "    \n",
    "    if print_logs:\n",
    "        print(f\"{datetime.now()} Test for epoch {epoch}: Class {class_name} Pixel AP {ap_pixel:.2f} Pixel AUC {auroc_pixel:.2f} Image AUC {auroc:.2f} Image AP {ap:.2f}\")\n",
    "    \n",
    "    del total_gt_pixel_scores\n",
    "    del total_pixel_scores\n",
    "    del anomaly_score_gt\n",
    "    del anomaly_score_prediction\n",
    "    del progress_bar\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return ap, ap_pixel, auroc, auroc_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasad.loss import SSIM\n",
    "from tasad.tasad_model import TasadModel\n",
    "\n",
    "\n",
    "progressbar_widgets = [\n",
    "    DynamicMessage('log', format = '{formatted_value}'),\n",
    "    Bar(marker='=', left='[', right=']'),\n",
    "    ' ',  ETA(),\n",
    "]\n",
    "\n",
    "def train_tasad_and_vitcnn(\n",
    "    class_name: str,\n",
    "    vitcnn,\n",
    "    epochs=500,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=1\n",
    "):\n",
    "    train_loader, test_loader = read_data(class_name)\n",
    "    \n",
    "    print(f\"\\n\\nStarting training for class: {class_name}\\n\")\n",
    "    print(f\"Info: Found {len(train_loader.dataset)} sample for training\")\n",
    "    print(f\"Info: Found {len(test_loader.dataset)} sample for test\")    \n",
    "\n",
    "    fas_model = TasadModel(in_channels=3, out_channels=1).cuda()\n",
    "    fas_parameters_amount = TasadModel.get_n_params(fas_model)\n",
    "    \n",
    "    print(f\"Info: Initializing fas with {fas_parameters_amount} parameters\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam([{\"params\": fas_model.parameters(), \"lr\": learning_rate}])\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [epochs*0.3, epochs*0.5, epochs*0.8], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_loss = 1e10\n",
    "    \n",
    "    summary_writer = SummaryWriter(log_dir=f'../runs/tasad-vitcnn/{class_name}')\n",
    "    \n",
    "    loss_l2 = torch.nn.modules.loss.MSELoss()\n",
    "    # SSIM = StructuralSimilarityIndexMeasure().cuda()\n",
    "    loss_ssim = SSIM(0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sum_loss = 0\n",
    "        sum_ssim_loss = 0\n",
    "        sum_l2_loss = 0\n",
    "        saved_in_this_epoch = False\n",
    "        with ProgressBar(widgets=progressbar_widgets, max_value=test_loader.__len__() + 1) as progress_bar:\n",
    "            for batch_index, batch in enumerate(test_loader):\n",
    "                input_batch = batch['augmented_image'].cuda()\n",
    "                ground_truth_batch = batch['anomaly_mask'].cuda()\n",
    "                has_anomaly = batch['has_anomaly'].cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                fas_input, vitcnn_reconstruction, vitcnn_ssim_value, vitcnn_ssim_map, vitcnn_binary_ssim_map = \\\n",
    "                    get_vitcnn_output_mask(input_batch, vitcnn)\n",
    "                \n",
    "                fas_output = fas_model(fas_input)\n",
    "                \n",
    "                vitcnn_binary_ssim_map = vitcnn_binary_ssim_map.to(fas_output.device)\n",
    "                \n",
    "                processed_fas_output = fas_output * vitcnn_binary_ssim_map.mean(dim=1, keepdim=True)\n",
    "                \n",
    "                processed_fas_output = torch.where(fas_output < 0.3, torch.zeros_like(processed_fas_output), torch.ones_like(processed_fas_output))\n",
    "                \n",
    "                if has_anomaly[0] and not saved_in_this_epoch:\n",
    "                    saved_in_this_epoch = True\n",
    "                    \n",
    "                    save_comparison(\n",
    "                        class_name=class_name,\n",
    "                        file_name=f\"train_sample_epoch_{epoch}.jpg\",\n",
    "                        image=input_batch,\n",
    "                        mask=ground_truth_batch,\n",
    "                        reconstruction=vitcnn_reconstruction,\n",
    "                        ssim_map=vitcnn_ssim_map,\n",
    "                        fas_input=fas_input,\n",
    "                        fas_output=fas_output,\n",
    "                        processed_fas_output=processed_fas_output,\n",
    "                        save_fig=True,\n",
    "                        summary_writer=summary_writer,\n",
    "                        epoch=epoch,\n",
    "                        path=f\"../runs/tasad-vitcnn/{class_name}/train\")\n",
    "                \n",
    "                l2_loss = loss_l2(fas_output, ground_truth_batch)\n",
    "                    \n",
    "                sum_l2_loss += l2_loss.cpu()\n",
    "                \n",
    "                ssim_loss = loss_ssim(fas_output, ground_truth_batch)\n",
    "                \n",
    "                sum_ssim_loss += ssim_loss.cpu()\n",
    "                \n",
    "                loss = l2_loss + ssim_loss\n",
    "                \n",
    "                sum_loss += loss.cpu()                \n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                progress_bar.update(\n",
    "                    batch_index, \n",
    "                    log=f\"({epoch+1}) Class: {class_name} | L2 loss: {(sum_l2_loss / (batch_index + 1)):.2f} | SSIM loss: {(sum_ssim_loss / (batch_index + 1)):.2f} | L2 and SSIM loss: {(sum_loss / (batch_index + 1)):.2f} \")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        del input_batch\n",
    "        del ground_truth_batch\n",
    "        del has_anomaly\n",
    "        del batch\n",
    "        del fas_input\n",
    "        del vitcnn_reconstruction\n",
    "        del vitcnn_ssim_value\n",
    "        del vitcnn_ssim_map\n",
    "        del vitcnn_binary_ssim_map\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        ap, ap_pixel, auroc, auroc_pixel = test_segmentation_model(\n",
    "            vitcnn=vitcnn,\n",
    "            class_name=class_name,\n",
    "            dataloader=test_loader,\n",
    "            epoch=epoch,\n",
    "            gpu_id=0,\n",
    "            fas_model=fas_model,\n",
    "            visualizer=summary_writer\n",
    "        )\n",
    "        \n",
    "        fas_model.train()\n",
    "            \n",
    "        print(f\"(test) AP: {ap:.2f} | AP pixel: {ap_pixel:.2f} | AUROC: {auroc:.2f} AUROC pixel: {auroc_pixel:.2f}\")\n",
    "        \n",
    "        summary_writer.add_scalar('fas_l2_loss', sum_l2_loss / len(train_loader), epoch)\n",
    "        summary_writer.add_scalar('fas_ssim_loss', sum_ssim_loss / len(train_loader), epoch)\n",
    "        \n",
    "        avg_loss = sum_loss / len(train_loader)\n",
    "        if avg_loss < best_loss and best_loss - avg_loss >= 0.01:\n",
    "            best_loss = avg_loss\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            torch.save(fas_model.state_dict(), os.path.join(f\"../tasad_models/tasad_{class_name}.pt\"))\n",
    "        elif (epoch + 1) - best_epoch >= 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vit_model(class_name: str):\n",
    "    model = Model(patch_size=16, depth=32).cuda()\n",
    "    model.load_state_dict(torch.load(f\"../vit_models/reconstruction/vit_{class_name}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_class(class_name: str):\n",
    "    vit_model = get_vit_model(class_name)\n",
    "\n",
    "    train_tasad_and_vitcnn(class_name, vit_model, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting training for class: bottle\n",
      "\n",
      "Info: Found 209 sample for training\n",
      "Info: Found 83 sample for test\n",
      "Info: Initializing fas with 11959681 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evry/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/conv.py:952: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1712609048481/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv_transpose2d(\n",
      "/home/evry/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712609048481/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "(1) Class: bottle | L2 loss: 0.10 | SSIM loss: 0.91 | L2 and SSIM loss: 1.01 [] Time:  0:00:24\n",
      "test: (82/83) Class bottle [===================================] Time:  0:00:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-12 08:08:38.227907 Test for epoch 0: Class bottle Pixel AP 0.06 Pixel AUC 0.39 Image AUC 0.58 Image AP 0.83\n",
      "(test) AP: 0.83 | AP pixel: 0.06 | AUROC: 0.58 AUROC pixel: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(2) Class: bottle | L2 loss: 0.07 | SSIM loss: 0.54 | L2 and SSIM loss: 0.60 [] Time:  0:00:24\n",
      "test: (52/83) Class bottle [=====================              ] ETA:   0:00:05\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_class(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbottle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mtrain_class\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_class\u001b[39m(class_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      2\u001b[0m     vit_model \u001b[38;5;241m=\u001b[39m get_vit_model(class_name)\n\u001b[0;32m----> 4\u001b[0m     train_tasad_and_vitcnn(class_name, vit_model, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 117\u001b[0m, in \u001b[0;36mtrain_tasad_and_vitcnn\u001b[0;34m(class_name, vitcnn, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m vitcnn_binary_ssim_map\n\u001b[1;32m    115\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m--> 117\u001b[0m ap, ap_pixel, auroc, auroc_pixel \u001b[38;5;241m=\u001b[39m test_segmentation_model(\n\u001b[1;32m    118\u001b[0m     vitcnn\u001b[38;5;241m=\u001b[39mvitcnn,\n\u001b[1;32m    119\u001b[0m     class_name\u001b[38;5;241m=\u001b[39mclass_name,\n\u001b[1;32m    120\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m    121\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    122\u001b[0m     gpu_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    123\u001b[0m     fas_model\u001b[38;5;241m=\u001b[39mfas_model,\n\u001b[1;32m    124\u001b[0m     visualizer\u001b[38;5;241m=\u001b[39msummary_writer\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m fas_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(test) AP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00map\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | AP pixel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00map_pixel\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | AUROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauroc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m AUROC pixel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauroc_pixel\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m, in \u001b[0;36mtest_segmentation_model\u001b[0;34m(vitcnn, class_name, dataloader, epoch, gpu_id, fas_model, visualizer, print_logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m ground_truth_mask_np \u001b[38;5;241m=\u001b[39m ground_truth_mask\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m, :, :, :]\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     77\u001b[0m fas_input, vitcnn_reconstruction, vitcnn_ssim_value, vitcnn_ssim_map, vitcnn_binary_ssim_map \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     78\u001b[0m         get_vitcnn_output_mask(vitcnn_input, vitcnn)\n\u001b[0;32m---> 80\u001b[0m fas_output \u001b[38;5;241m=\u001b[39m fas_model(fas_input)\n\u001b[1;32m     82\u001b[0m vitcnn_binary_ssim_map \u001b[38;5;241m=\u001b[39m vitcnn_binary_ssim_map\u001b[38;5;241m.\u001b[39mto(fas_output\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     84\u001b[0m processed_fas_output \u001b[38;5;241m=\u001b[39m fas_output \u001b[38;5;241m*\u001b[39m vitcnn_binary_ssim_map\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/master-degree/repositories/vision-anomaly/src/tasad/tasad_model.py:46\u001b[0m, in \u001b[0;36mTasadModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mForward pass through the network.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    Output tensor.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m b3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 46\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(b3)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/master-degree/repositories/vision-anomaly/src/tasad/tasad_model.py:228\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, b3)\u001b[0m\n\u001b[1;32m    226\u001b[0m db1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb1(up1)\n\u001b[1;32m    227\u001b[0m up2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(db1)\n\u001b[0;32m--> 228\u001b[0m db2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb2(up2)\n\u001b[1;32m    229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_out(db2)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-anomaly/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_class(\"bottle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
